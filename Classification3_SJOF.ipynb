{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Task: KNN\n",
    "I will be building off of the mini-projct and build out a KNN classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "# import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import sklearn\n",
    "# import seaborn as sns\n",
    "warnings.simplefilter('ignore', DeprecationWarning)\n",
    "warnings.simplefilter('ignore', FutureWarning)\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn import preprocessing\n",
    "from sklearn import metrics as mt\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path = \"/home/kyle/Documents/thomaskh522@gmail.com/SMU/DataMining/Classificaiton2/\"\n",
    "#scada =  pd.read_csv(path + \"SCADAcombined2017_T014.csv\")\n",
    "#alarms_desc = pd.read_csv(path + \"Siemens_Fault_Code_Descriptions.csv\")\n",
    "scada =  pd.read_csv(\"SCADAcombined2017_T014.csv\")\n",
    "alarms_desc = pd.read_csv(\"Siemens_Fault_Code_Descriptions.csv\")\n",
    "print(\"DF scada is: \", scada.shape)\n",
    "scada.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning data and some feature engineering\n",
    "This code is all borrowed from the mini-project. The goal is to recreate the dataset as it was in the mini-project, but for the entire year. There was some missing and infinite data in the dataset. These values were transformed into NaN and dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scada.dropna(thresh=10000,axis=1,inplace=True)\n",
    "\n",
    "colsdrop = ['StationId', 'CommunicationId', 'NeighbourId',\n",
    "           'NeighbourIdTwo', 'XPos', 'YPos', 'wtc_ActPower_min',\n",
    "           'wtc_ActPower_max', 'wtc_ActPower_stddev',\n",
    "           'wtc_AmpPhR_min', 'wtc_AmpPhR_max', 'wtc_AmpPhR_mean',\n",
    "           'wtc_AmpPhR_stddev', 'wtc_AmpPhS_min', 'wtc_AmpPhS_max',\n",
    "           'wtc_AmpPhS_mean', 'wtc_AmpPhS_stddev', 'wtc_AmpPhT_min',\n",
    "           'wtc_AmpPhT_max', 'wtc_AmpPhT_mean', 'wtc_AmpPhT_stddev',\n",
    "           'wtc_RawPower_min', 'wtc_RawPower_max', 'wtc_RawPower_mean',\n",
    "           'wtc_RawPower_stddev', 'wtc_GenRpm_mean',\n",
    "           'wtc_MainSRpm_mean', 'wtc_PitchRef_BladeA_mean',\n",
    "           'wtc_PitchRef_BladeB_mean', 'wtc_PitchRef_BladeC_mean','wtc_TwrHumid_mean', \n",
    "            'wtc_PitcPosA_mean','wtc_PitcPosB_mean', 'wtc_PitcPosC_mean','wtc_BoostKWh_endvalue',\n",
    "           'wtc_GeOilTmp_mean','wtc_HubTemp_mean','wtc_NacelTmp_mean','wtc_ConvWTmp_mean',\n",
    "           'wtc_MainBTmp_mean','WindDir_Angle_3D_Avg','WindDir_Angle_3D_Std','WindDir_Angle_3D_Avg',\n",
    "           'WindDir_Angle_3D_Std']\n",
    "\n",
    "scada.drop(colsdrop,axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scada.dropna(inplace=True)\n",
    "scada['wtc_AlarmCde_endvalue'] = scada['wtc_AlarmCde_endvalue'].astype(int)\n",
    "\n",
    "df = scada.merge(alarms_desc, \n",
    "         left_on = 'wtc_AlarmCde_endvalue', \n",
    "         right_on=\"Code\",\n",
    "         how='left')\n",
    "\n",
    "# del alarms_desc\n",
    "\n",
    "df['TimeStamp'] = pd.to_datetime(df['TimeStamp'])\n",
    "df.set_index('TimeStamp', inplace=True)\n",
    "df.index\n",
    "\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "print(df.shape)\n",
    "df.describe().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new variables\n",
    "\n",
    "# Rounded Wind Speed for GroupBy\n",
    "df['WindSpeed_adj'] = df['wtc_AcWindSp_mean'].round(1)\n",
    "\n",
    "# Wind changes at night\n",
    "df['HourOfDay'] = df.index.hour\n",
    "\n",
    "# Yaw Error\n",
    "df['YawError'] = df['wtc_YawPos_mean'] - df['WindDir_ICEFREE_D1_WVT']\n",
    "\n",
    "# Adding boost to ref set point\n",
    "df['BoostedRef'] = df['wtc_BoostAva_mean'] + df['wtc_PowerRef_endvalue']\n",
    "\n",
    "# Wind Sheer\n",
    "df['WindSheer1'] = np.log(df['WS_Thies_80m_Avg'] / df['WS_Thies_26m_Avg']) / np.log(80/26)\n",
    "# df['WindSheer2'] = np.log(df['WS_Thies_80m_Avg'] / df['WS_Thies_60m_Avg']) / np.log(80/60)\n",
    "\n",
    "# Fault Types\n",
    "df = df.rename(columns={'Fault Type': 'Fault_Type'})\n",
    "df['Fault_Type'] = df['Fault_Type'].replace('W','0')\n",
    "df['Fault_Type'] = df['Fault_Type'].astype(int)\n",
    "\n",
    "# Boosted Power Delta\n",
    "df['PowerDelta'] = df['BoostedRef'] - df['wtc_ActPower_mean']\n",
    "\n",
    "available = (df['Fault_Type'] != 1) & (df['wtc_ScInOper_timeon'] == 600)\n",
    "df['Available'] = [1 if x == True else 0 for x in available]\n",
    "\n",
    "# Icing\n",
    "icing = (df['AirTC_3m_Avg'] < 3) & (abs(df['WS_ICEFREE_Avg'] - df['WS_Thies_80m_Avg']) > 1)\n",
    "df['Icing'] = [1 if x == True else 0 for x in icing]\n",
    "\n",
    "# Air Density\n",
    "df['Air_density'] = (df['BP_mbar_76m_Avg']*100)/((df['AirTC_3m_Avg']+273.15)*287.05)\n",
    "\n",
    "# Air Density adjusted Wind Speed\n",
    "df['AdjTurbineWS'] = df['wtc_AcWindSp_mean']*(np.power(df['Air_density'],(1/3)))\n",
    "\n",
    "# Turbulence\n",
    "df['TurbulenceMet'] = df['WS_Thies_80m_Std'] / df['WS_Thies_80m_Avg']\n",
    "df['TurbulenceTurbine'] = df['wtc_AcWindSp_stddev'] / df['wtc_AcWindSp_mean']\n",
    "\n",
    "# df['MainBearingFault'] = [1 if x == 64038 else 0 for x in df['Code']]\n",
    "\n",
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating Derate Conditions\n",
    "\n",
    "#derate = (df['wtc_ActPower_mean'] == df['wtc_PowerRef_endvalue']) & (df['wtc_PowerRef_endvalue'] < 2300)\n",
    "#derate = (df['wtc_PowerRef_endvalue'] < 2300)\n",
    "derate = (df['PowerDelta'] < 0) & (df['Available'] == 1) & (df['BoostedRef'] < 2300) #df['BoostedRef'].max())\n",
    "df['Derated'] = [1 if x == True else 0 for x in derate]\n",
    "\n",
    "percent = (df['Derated'].sum() / len(df))*100\n",
    "print('Percent of Derated Data:',percent,\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use cleaned data set for calculating expected energy\n",
    "clean = (df['Available'] == 1) & (df['wtc_PowerRef_endvalue'] == 2300) & (df['Icing'] == 0) & (df['wtc_BoostAva_mean'] == 0)\n",
    "df_clean = df[clean]\n",
    "pcurve95 = df_clean.groupby('WindSpeed_adj').quantile(q=0.95)\n",
    "pcurve75 = df_clean.groupby('WindSpeed_adj').quantile(q=0.75)\n",
    "pcurve50 = df_clean.groupby('WindSpeed_adj').median()\n",
    "pcurve25 = df_clean.groupby('WindSpeed_adj').quantile(q=0.25)\n",
    "pcurve5 = df_clean.groupby('WindSpeed_adj').quantile(q=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Historical 5th Percentile\n",
    "\n",
    "import pylab\n",
    "from scipy.optimize import curve_fit\n",
    "#from sklearn import preprocessing\n",
    "\n",
    "def sigmoid(x, x0, k):\n",
    "    #cap = 2300\n",
    "    y = cap / (1 + np.exp(-k*(x-x0)))\n",
    "    return y\n",
    "\n",
    "xdata = pcurve5.index.values\n",
    "ydata = pcurve5['wtc_ActPower_mean'].values\n",
    "cap = np.max(pcurve5['wtc_ActPower_mean'].values)\n",
    "\n",
    "popt, pcov = curve_fit(sigmoid, xdata, ydata)\n",
    "\n",
    "pcurve5_x0 = popt[0]\n",
    "pcurve5_k = popt[1]\n",
    "pcurve5_cap = cap\n",
    "print('x0:',pcurve5_x0)\n",
    "print('k:',pcurve5_k)\n",
    "print('Cap:',pcurve5_cap)\n",
    "\n",
    "x = xdata\n",
    "y = sigmoid(x,*popt)\n",
    "\n",
    "pylab.plot(xdata, ydata, 'o', label='data')\n",
    "pylab.plot(xdata,y, label='fit')\n",
    "pylab.ylim(0, 3000)\n",
    "pylab.legend(loc='best')\n",
    "pylab.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PowerCurve(wind, x0,k,cap):\n",
    "    y = cap / (1 + np.exp(-k*(wind-x0)))\n",
    "    return y\n",
    "\n",
    "df['ExpectedEnergy_ref'] = df.apply(lambda x: PowerCurve(wind=x['wtc_AcWindSp_mean'], \n",
    "                                                          x0=7.70514731, \n",
    "                                                          k=0.85471648, \n",
    "                                                          cap=2300), \n",
    "                                     axis = 1)\n",
    "df['ExpectedEnergy_5th'] = df.apply(lambda x: PowerCurve(wind=x['wtc_AcWindSp_mean'], \n",
    "                                                          x0=pcurve5_x0, \n",
    "                                                          k=pcurve5_k,\n",
    "                                                          cap=pcurve5_cap), \n",
    "                                     axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make labels\n",
    "# insipred by: https://stackoverflow.com/questions/26886653/pandas-create-new-column-based-on-values-from-other-columns\n",
    "\n",
    "def make_class(row):\n",
    "    if row[\"wtc_ActPower_mean\"] <= row[\"ExpectedEnergy_5th\"]:\n",
    "        return \"1\"\n",
    "    else:\n",
    "        return \"0\"\n",
    "\n",
    "df[\"Underperformance\"] = df.apply(lambda x: make_class(x), axis=1)\n",
    "df['Underperformance'] = df['Underperformance'].astype(int)\n",
    "\n",
    "percent = (df['Underperformance'].sum() / len(df))*100\n",
    "print('Percent of Under Performance Data:',percent,\"%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encode the labels (examples found on SKLearn website... need to find address)\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(list(df[\"Underperformance\"].unique()))\n",
    "# df_test[\"target\"] = le.transform(df_test[\"Underperformance\"])\n",
    "df[\"target\"] = le.transform(df[\"Underperformance\"])\n",
    "df = df.replace([np.inf, -np.inf], np.nan).dropna()\n",
    "\n",
    "\n",
    "\n",
    "#Now the other variables used to train the model need to scaled. First, however, un-need variables need to be dropped.\n",
    "y = df[\"target\"]\n",
    "# y2= df['target']\n",
    "\n",
    "\n",
    "\n",
    "# df3 = df.drop(['wtc_GeOilTmp_mean',\n",
    "#        'wtc_HubTemp_mean', 'wtc_NacelTmp_mean', 'wtc_ConvWTmp_mean',\n",
    "#        'wtc_HydOilTm_mean', 'wtc_MainBTmp_mean', 'wtc_AlarmCde_endvalue',\n",
    "#        'wtc_ScInOper_timeon', 'WindSpeed_adj','ExpectedEnergy_ref', 'ExpectedEnergy_25th',\n",
    "#        'ExpectedEnergy_median', 'Underperformance','wtc_ActPower_mean','Fault_Type','Code', \n",
    "#         'Description', 'Fault_Type','Name','Derated','target'], axis=1)\n",
    "\n",
    "# keepcols2 = [\"Air_density\", \"wtc_ScInOper_timeon\", \"AirTC_3m_Avg\", \"WS_Thies_60m_Avg\", \n",
    "#              \"WS_ICEFREE_Avg\", \"BP_mbar_76m_Avg\"]\n",
    "\n",
    "keepcols = ['BoostedRef', 'WindSheer1',\n",
    "            'wtc_AcWindSp_mean','wtc_NacelPos_mean','wtc_ScInOper_timeon', \n",
    "            'WS_Thies_80m_Avg', 'WS_Thies_60m_Avg', 'WS_ICEFREE_Avg',\n",
    "           'BP_mbar_76m_Avg', 'AirTC_3m_Avg','Icing','Air_density']\n",
    "\n",
    "data = df[keepcols]\n",
    "\n",
    "\n",
    "X = preprocessing.scale(data) \n",
    "# X2 = preprocessing.scale(df3)\n",
    "\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start KNN Classifier\n",
    "Everything prior to this was data preperation. \n",
    "\n",
    "The goal of this classification model was to predict turbine under performance using a KNN approach. This information would be helpful to identify if a turbine is under performing using historical data. \n",
    "\n",
    "I did two things here. First, I ran a really simple KNN using different K values. Then I did a intensive grid search. My accuracy did not improve that much even though I trained and tested 30 different models. \n",
    "\n",
    "**Best Model: {'metric': 'manhattan', 'n_neighbors': 5, 'weights': 'distance'}**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in range(1,11):\n",
    "    K=x\n",
    "    clf = KNeighborsClassifier(n_neighbors=K, weights='uniform', metric='euclidean')\n",
    "    clf.fit(X_train,y_train)\n",
    "    p = clf.predict(X_test)\n",
    "    a = accuracy_score(p,y_test)\n",
    "    print('Accuracy of classifier with %d neighbors is: %.2f'%(K,a))\n",
    "# I was getting 92% accuracy just through some basic KNN... I don't think we will get much better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "k = list(range(1,6)) # search up to 5 neighbors\n",
    "w = ['uniform', 'distance']\n",
    "m = ['euclidean', 'minkowski', 'manhattan']\n",
    "parameter_grid = {'n_neighbors': k, 'weights' : w, 'metric': m}\n",
    "\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "g_seach = GridSearchCV(knn, param_grid=parameter_grid, scoring='accuracy')\n",
    "g_seach.fit(X_train, y_train)\n",
    "print(\"Best parameters set found on development set:\")\n",
    "print()\n",
    "print(g_seach.best_params_)\n",
    "print()\n",
    "\n",
    "print(\"Detailed classification report:\")\n",
    "print()\n",
    "\n",
    "y_true, y_pred = y_test, g_seach.predict(X_test)\n",
    "acc = accuracy_score(y_test,y_pred)\n",
    "print(classification_report(y_true, y_pred))\n",
    "print()\n",
    "print(\"Accuracy:\", acc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression Task - KNN Regression to Predict Power Output\n",
    "\n",
    "## Attempt 1: KNN Regression using non transformed data\n",
    "Energy output is of critical importance to a wind farm. Our group wanted to determine if we could predict power output using KNN regression. We used the best classifier from the grid search to generate the hyper-parameters used by the regressor. The intial results were excellent. We acheived an R2 of 0.9317 indicating that the model could explain a little more than 93 percent of the variation found in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y2 = df[\"wtc_ActPower_mean\"]\n",
    "\n",
    "# df3 = df.drop(['wtc_GeOilTmp_mean',\n",
    "#        'wtc_HubTemp_mean', 'wtc_NacelTmp_mean', 'wtc_ConvWTmp_mean',\n",
    "#        'wtc_HydOilTm_mean', 'wtc_MainBTmp_mean', 'wtc_AlarmCde_endvalue',\n",
    "#        'wtc_ScInOper_timeon', 'WindSpeed_adj','ExpectedEnergy_ref', 'ExpectedEnergy_25th',\n",
    "#        'ExpectedEnergy_median', 'Underperformance','wtc_ActPower_mean','Fault_Type','Code', \n",
    "#         'Description', 'Fault_Type','Name','Derated','target'], axis=1)\n",
    "\n",
    "# keepcols2 = [\"Air_density\", \"wtc_ScInOper_timeon\", \"AirTC_3m_Avg\", \"WS_Thies_60m_Avg\", \n",
    "#              \"WS_ICEFREE_Avg\", \"BP_mbar_76m_Avg\"]\n",
    "\n",
    "keepcols = ['WindSheer1',\n",
    "            'wtc_AcWindSp_mean','wtc_NacelPos_mean','wtc_ScInOper_timeon', \n",
    "            'WS_Thies_80m_Avg', 'WS_Thies_60m_Avg', 'WS_ICEFREE_Avg',\n",
    "           'BP_mbar_76m_Avg', 'AirTC_3m_Avg','Icing','Air_density']\n",
    "\n",
    "data2 = df[keepcols]\n",
    "\n",
    "\n",
    "# X2 = preprocessing.scale(data2)\n",
    "\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(data2, y2, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "reg = KNeighborsRegressor(n_neighbors=5, metric='manhattan', weights='distance')\n",
    "# reg = KNeighborsRegressor()\n",
    "reg.fit(X_train2, y_train2)\n",
    "pred2 = reg.predict(X_test2)\n",
    "\n",
    "reg.score(X_test2, y_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x=X_test2[\"wtc_AcWindSp_mean\"], y=y_test2)\n",
    "plt.plot(X_test2[\"wtc_AcWindSp_mean\"], pred2, 'r-')\n",
    "plt.axis('tight')\n",
    "plt.legend()\n",
    "plt.title(\"KNN Regression\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attempt 2 - Implementation of PCA\n",
    "PCA allowed us to reduce the features used in the KNN regression while maintaining a high R2 value. We did experience some interesting results. \n",
    "\n",
    "The first five principal components explaint over 99% of the variance in the data. However, the resulting KNN regression had an R2 of 0.8404. \n",
    "\n",
    "Using the first 6 principle components did not explain much more variance in the data, however, there was a large increase in model fit. The R2 increased to 0.9356 or a gain of 0.0952.\n",
    "\n",
    "This means that PCA KNN Regression yielded slightly better results with roughly half of the number of featuers using the same hyper parameters. However, this additional increase in model performance is not worth the decrease in interpretability from the regular model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using 5 Principal components\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca2 = PCA(n_components=5) # there are 11 features\n",
    "pca2.fit(data2)\n",
    "\n",
    "pca_X_train2 = pca2.transform(X_train2)\n",
    "pca_X_test2 = pca2.transform(X_test2)\n",
    "\n",
    "\n",
    "print(pca2.explained_variance_ratio_)\n",
    "print(sum(pca2.explained_variance_ratio_))\n",
    "\n",
    "reg_pca = KNeighborsRegressor(n_neighbors=5, metric='manhattan', weights='distance')\n",
    "reg_pca.fit(pca_X_train2, y_train2)\n",
    "pred_pca = reg_pca.predict(pca_X_test2)\n",
    "\n",
    "reg_pca.score(pca_X_test2, y_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using 6 principal components\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca2 = PCA(n_components=6) # there are  11 features\n",
    "pca2.fit(data2)\n",
    "\n",
    "pca_X_train2 = pca2.transform(X_train2)\n",
    "pca_X_test2 = pca2.transform(X_test2)\n",
    "\n",
    "\n",
    "print(pca2.explained_variance_ratio_)\n",
    "print(sum(pca2.explained_variance_ratio_))\n",
    "\n",
    "reg_pca = KNeighborsRegressor(n_neighbors=5, metric='manhattan', weights='distance')\n",
    "reg_pca.fit(pca_X_train2, y_train2)\n",
    "pred_pca = reg_pca.predict(pca_X_test2)\n",
    "\n",
    "reg_pca.score(pca_X_test2, y_test2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Another PCA implementation. Using regression for classificaiton... not wise, but left in for reference. \n",
    "\n",
    "# Do not include in final paper\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2) # 12 components for 12 features\n",
    "pca.fit(X)\n",
    "\n",
    "pca_X_train = pca.transform(X_train)\n",
    "pca_X_test = pca.transform(X_test)\n",
    "\n",
    "\n",
    "print(pca.explained_variance_ratio_)\n",
    "print(sum(pca.explained_variance_ratio_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg2 = KNeighborsRegressor(n_neighbors=5, metric='manhattan', weights='distance')\n",
    "reg2.fit(pca_X_train, y_train)\n",
    "pred2 = reg2.predict(pca_X_test)\n",
    "\n",
    "for x in range(1,11):\n",
    "    num = x / 10\n",
    "    test = pd.DataFrame(pred2)\n",
    "    test[\"Model\"] = test[0].apply(lambda x: 1 if x > num else 0)\n",
    "\n",
    "\n",
    "    holding = y_train.reset_index()\n",
    "\n",
    "    comparison = test.join(holding)\n",
    "    comparison[\"Correct\"] = comparison[\"Model\"] == comparison[\"target\"]\n",
    "    print(\"Accuracy for Regressor Model when cutoff is set at \" + str(num) + \" percent match:\", \n",
    "          (comparison[\"Correct\"].sum()/len(comparison[\"Correct\"])) * 100, \n",
    "          \" percent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Task: Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.externals.six import StringIO  \n",
    "from IPython.display import Image  \n",
    "from sklearn.tree import export_graphviz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print ('Dataset Lenght:: ' , len(tree))\n",
    "#tree.columns\n",
    "\n",
    "keepcols = ['WindSheer1',\n",
    "            'wtc_NacelPos_mean','wtc_ScInOper_timeon', \n",
    "            'WS_Thies_80m_Avg', 'WS_Thies_60m_Avg', 'WS_ICEFREE_Avg',\n",
    "           'BP_mbar_76m_Avg', 'AirTC_3m_Avg','Icing','Air_density']\n",
    "\n",
    "target = df[\"wtc_ActPower_mean\"]\n",
    "treedata = df[keepcols]\n",
    "print ('Dataset Shape:: ', tree.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to split the dataset\n",
    "def splitdataset(balance_data):\n",
    "  \n",
    "    x= treedata\n",
    "    y= target\n",
    "    # Seperating the target variable\n",
    "    X_train3, X_test3, y_train3, y_test3 = train_test_split(x, y, test_size = 0.3, random_state = 100)\n",
    "    return x, y, X_train3, X_test3, y_train3, y_test3\n",
    "print (x)\n",
    "print (y)\n",
    "print (X_train3)\n",
    "print (X_test3)\n",
    "print (y_train3)\n",
    "print (y_test3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform training with giniIndex.\n",
    "def train_using_gini(X_train3, X_test3, y_train3):\n",
    " \n",
    "    # Creating the classifier objec3t\n",
    "    clf_gini = DecisionTreeClassifier(criterion = \"gini\", random_state = 100,max_depth=5, min_samples_leaf=10)\n",
    " \n",
    "    # Performing training\n",
    "    clf_gini.fit(X_train3, y_train3)\n",
    "    return clf_gini\n",
    "print (clf_gini)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform training with entropy.\n",
    "def tarin_using_entropy(X_train3, X_test3, y_train3):\n",
    " \n",
    "    # Decision tree with entropy\n",
    "    clf_entropy = DecisionTreeClassifier(criterion = \"entropy\", random_state = 100, max_depth = 3, min_samples_leaf = 5)\n",
    " \n",
    "    # Performing training\n",
    "    clf_entropy.fit(X_train3, y_train3)\n",
    "    return clf_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to make predictions\n",
    "def prediction(X_test3, clf_object):\n",
    " \n",
    "    # Predicton on test with giniIndex\n",
    "    y_pred = clf_object.predict(X_test3)\n",
    "    print(\"Predicted values: \")\n",
    "    print(y_pred)\n",
    "    return y_pred\n",
    "print(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate accuracy\n",
    "def cal_accuracy(y_test3, y_pred3):\n",
    "     \n",
    "    print(\"Confusion Matrix: \",\n",
    "        confusion_matrix(y_test3, y_pred3))\n",
    "     \n",
    "    print (\"Accuracy : \",\n",
    "    accuracy_score(y_test3,y_pred3)*100)\n",
    "     \n",
    "    print(\"Report : \",\n",
    "    classification_report(y_test3, y_pred3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Driver code\n",
    "def main():\n",
    "     \n",
    "    # Building Phase\n",
    "    #tree = importdata()\n",
    "    treedata, target, X_train3, X_test3, y_train3, y_test3 = splitdataset(treedata)\n",
    "    clf_gini = train_using_gini(X_train3, X_test3, y_train3)\n",
    "    clf_entropy = tarin_using_entropy(X_train3, X_test3, y_train3)\n",
    "     \n",
    "    # Operational Phase\n",
    "    print(\"Results Using Gini Index:\")\n",
    "     \n",
    "    # Prediction using gini\n",
    "    y_pred_gini = prediction(X_test3, clf_gini)\n",
    "    cal_accuracy(y_test3, y_pred_gini)\n",
    "     \n",
    "    print(\"Results Using Entropy:\")\n",
    "    # Prediction using entropy\n",
    "    y_pred_entropy = prediction(X_test3, clf_entropy)\n",
    "    cal_accuracy(y_test3, y_pred_entropy)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Calling main function\n",
    "if __name__==\"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
